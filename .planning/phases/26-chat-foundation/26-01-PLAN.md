# 26-01: AI Infrastructure

## Objective

Set up the AI SDK multi-provider infrastructure with automatic fallback, streaming API route, and provider configuration. This establishes the foundation for all chat functionality.

## Execution Context

```yaml
references:
  - .planning/phases/26-chat-foundation/26-CONTEXT.md
  - .planning/phases/26-chat-foundation/26-RESEARCH.md
  - .planning/FRONTEND_DESIGN_SKILL.md
  - v2/docs/DESIGN_SYSTEM.md
  - v2/CLAUDE.md

patterns:
  - v2/src/app/api/google/connect/route.ts  # API route pattern
  - v2/src/lib/google/oauth.ts               # External lib pattern
```

## Context

### What We're Building
- Multi-provider fallback chain with 5+ free LLM providers
- Streaming API route (`/api/chat`) with authentication
- Provider configuration module for easy management
- System prompt foundation for PERM chatbot

### Key Decisions (from 26-CONTEXT.md & 26-RESEARCH.md)
- **AI SDK 5.x** with `ai-fallback` library for automatic provider switching
- **Next.js API routes** for streaming (not Convex HTTP actions - not fully supported)
- **Authenticated-only** - API route verifies user is logged in
- **Provider chain**: Gemini 2.0 Flash → DeepSeek → Llama 3.3 70B → Groq → Qwen

### Dependencies
- New packages: `ai`, `@ai-sdk/react`, `@ai-sdk/google`, `@ai-sdk/openai`, `@openrouter/ai-sdk-provider`, `ai-fallback`
- Environment variables for API keys (Gemini, OpenRouter, Groq)

## Tasks

### Task 1: Install AI SDK Dependencies

```bash
# Install core AI SDK packages
pnpm add ai@^5 @ai-sdk/react @ai-sdk/google @ai-sdk/openai @openrouter/ai-sdk-provider ai-fallback@^1
```

**Verify:** `pnpm list ai @ai-sdk/react` shows version 5.x

### Task 2: Create Provider Configuration

**File:** `v2/src/lib/ai/providers.ts`

```typescript
/**
 * AI Provider Configuration
 *
 * Multi-provider fallback chain for chat functionality.
 * Uses free-tier LLMs with automatic failover.
 *
 * Provider priority:
 * 1. Gemini 2.0 Flash (fastest, smartest free)
 * 2. DeepSeek via OpenRouter (excellent quality)
 * 3. Llama 3.3 70B via OpenRouter (reliable)
 * 4. Groq Llama 3.3 70B (fastest inference)
 * 5. Qwen 32B via OpenRouter (fallback)
 */

import { createFallback } from 'ai-fallback';
import { google } from '@ai-sdk/google';
import { createOpenRouter } from '@openrouter/ai-sdk-provider';
import { createOpenAI } from '@ai-sdk/openai';

// OpenRouter client (300+ models, many free)
const openrouter = createOpenRouter({
  apiKey: process.env.OPENROUTER_API_KEY!,
});

// Groq client (uses OpenAI-compatible API)
const groq = createOpenAI({
  baseURL: 'https://api.groq.com/openai/v1',
  apiKey: process.env.GROQ_API_KEY!,
});

/**
 * Chat model with automatic fallback across providers
 *
 * Fallback triggers on:
 * - Rate limit errors (429)
 * - Server errors (5xx)
 * - Timeout errors
 *
 * After failure, retries primary after modelResetInterval
 */
export const chatModel = createFallback({
  models: [
    // Tier 1: Primary (smartest, fastest)
    google('gemini-2.0-flash'),

    // Tier 2: Strong fallbacks
    openrouter('deepseek/deepseek-chat-v3-0324:free'),
    openrouter('meta-llama/llama-3.3-70b-instruct:free'),

    // Tier 3: Speed fallbacks
    groq('llama-3.3-70b-versatile'),

    // Tier 4: Emergency
    openrouter('qwen/qwq-32b:free'),
  ],
  onError: (error, modelId) => {
    console.error(`[AI Fallback] Provider ${modelId} failed:`, error.message);
  },
  modelResetInterval: 60_000, // Try primary again after 1 minute
});

/**
 * List of supported models for debugging/UI
 */
export const SUPPORTED_MODELS = [
  { id: 'gemini-2.0-flash', name: 'Gemini 2.0 Flash', provider: 'Google' },
  { id: 'deepseek-chat-v3', name: 'DeepSeek V3', provider: 'OpenRouter' },
  { id: 'llama-3.3-70b', name: 'Llama 3.3 70B', provider: 'OpenRouter' },
  { id: 'llama-3.3-70b', name: 'Llama 3.3 70B', provider: 'Groq' },
  { id: 'qwq-32b', name: 'Qwen QwQ 32B', provider: 'OpenRouter' },
] as const;
```

**Verify:** File exists with proper exports

### Task 3: Create System Prompt

**File:** `v2/src/lib/ai/system-prompt.ts`

```typescript
/**
 * System Prompt for PERM Tracker Chatbot
 *
 * Phase 26 = Foundation only. Just basic helpful assistant.
 * Phase 27 = PERM knowledge, website how-tos
 * Phase 28 = Tool calling, actions
 */

export const SYSTEM_PROMPT = `You are a helpful assistant for PERM Tracker, an immigration case management application.

Your role is to help users understand the PERM labor certification process and assist with their cases.

Guidelines:
- Be concise and accurate
- Use professional language appropriate for legal/immigration context
- If you don't know something, say so rather than guessing
- Focus on being helpful and direct

Current capabilities:
- Answer general questions about PERM process
- Provide guidance on using the application
- Have friendly conversations

Note: You cannot yet access case data or take actions. These features are coming soon.`;

/**
 * Get the system prompt (can be customized per user/context later)
 */
export function getSystemPrompt(): string {
  return SYSTEM_PROMPT;
}
```

### Task 4: Create Streaming API Route

**File:** `v2/src/app/api/chat/route.ts`

```typescript
/**
 * Chat Streaming API Route
 *
 * Handles AI chat requests with:
 * - Authentication verification
 * - Multi-provider fallback
 * - Streaming response
 *
 * POST /api/chat
 * Body: { messages: UIMessage[] }
 * Returns: Streaming response (AI SDK format)
 */

import { streamText, UIMessage, convertToModelMessages } from 'ai';
import { isAuthenticatedNextjs } from '@convex-dev/auth/nextjs/server';
import { chatModel } from '@/lib/ai/providers';
import { getSystemPrompt } from '@/lib/ai/system-prompt';

// Allow up to 30 seconds for streaming responses
export const maxDuration = 30;

export async function POST(req: Request) {
  try {
    // Verify authentication (chatbot is authenticated-only)
    const isAuthenticated = await isAuthenticatedNextjs();
    if (!isAuthenticated) {
      return new Response(
        JSON.stringify({ error: 'Unauthorized' }),
        { status: 401, headers: { 'Content-Type': 'application/json' } }
      );
    }

    // Parse request body
    const { messages }: { messages: UIMessage[] } = await req.json();

    if (!messages || !Array.isArray(messages)) {
      return new Response(
        JSON.stringify({ error: 'Invalid request: messages required' }),
        { status: 400, headers: { 'Content-Type': 'application/json' } }
      );
    }

    // Stream the response
    const result = streamText({
      model: chatModel,
      system: getSystemPrompt(),
      messages: await convertToModelMessages(messages),
      maxTokens: 2000,
    });

    return result.toUIMessageStreamResponse();
  } catch (error) {
    console.error('[Chat API] Error:', error);
    return new Response(
      JSON.stringify({ error: 'Failed to process chat request' }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
}
```

**Verify:** API route responds to POST requests

### Task 5: Create Index Export

**File:** `v2/src/lib/ai/index.ts`

```typescript
/**
 * AI Module Exports
 *
 * Central export point for AI-related utilities
 */

export { chatModel, SUPPORTED_MODELS } from './providers';
export { SYSTEM_PROMPT, getSystemPrompt } from './system-prompt';
```

### Task 6: Add Environment Variables Template

**Update:** `v2/.env.example` (create if doesn't exist)

```bash
# AI Provider API Keys (required for chat functionality)
# Get keys from:
# - Google AI Studio: https://aistudio.google.com/
# - OpenRouter: https://openrouter.ai/keys
# - Groq: https://console.groq.com/keys

GOOGLE_GENERATIVE_AI_API_KEY=your_gemini_api_key
OPENROUTER_API_KEY=your_openrouter_api_key
GROQ_API_KEY=your_groq_api_key
```

**User action required:** Add actual API keys to `.env.local` and Convex dashboard

### Task 7: Write Tests for Provider Configuration

**File:** `v2/src/lib/ai/__tests__/providers.test.ts`

```typescript
import { describe, it, expect, vi } from 'vitest';

// Mock the external providers
vi.mock('@ai-sdk/google', () => ({
  google: vi.fn((model) => ({ provider: 'google', model })),
}));

vi.mock('@openrouter/ai-sdk-provider', () => ({
  createOpenRouter: vi.fn(() => vi.fn((model) => ({ provider: 'openrouter', model }))),
}));

vi.mock('@ai-sdk/openai', () => ({
  createOpenAI: vi.fn(() => vi.fn((model) => ({ provider: 'groq', model }))),
}));

vi.mock('ai-fallback', () => ({
  createFallback: vi.fn(({ models }) => ({
    type: 'fallback',
    modelCount: models.length,
  })),
}));

describe('AI Providers', () => {
  it('exports chatModel', async () => {
    const { chatModel } = await import('../providers');
    expect(chatModel).toBeDefined();
    expect(chatModel.type).toBe('fallback');
  });

  it('has multiple fallback models', async () => {
    const { chatModel } = await import('../providers');
    expect(chatModel.modelCount).toBeGreaterThanOrEqual(4);
  });

  it('exports SUPPORTED_MODELS list', async () => {
    const { SUPPORTED_MODELS } = await import('../providers');
    expect(SUPPORTED_MODELS).toBeDefined();
    expect(SUPPORTED_MODELS.length).toBeGreaterThan(0);
  });
});

describe('System Prompt', () => {
  it('exports system prompt', async () => {
    const { SYSTEM_PROMPT, getSystemPrompt } = await import('../system-prompt');
    expect(SYSTEM_PROMPT).toBeDefined();
    expect(typeof SYSTEM_PROMPT).toBe('string');
    expect(SYSTEM_PROMPT.length).toBeGreaterThan(100);
  });

  it('getSystemPrompt returns prompt', async () => {
    const { getSystemPrompt, SYSTEM_PROMPT } = await import('../system-prompt');
    expect(getSystemPrompt()).toBe(SYSTEM_PROMPT);
  });
});
```

**Verify:** `pnpm test src/lib/ai/__tests__/providers.test.ts` passes

## Verification

```bash
# 1. Packages installed correctly
pnpm list ai @ai-sdk/react ai-fallback

# 2. TypeScript compiles
pnpm build

# 3. Tests pass
pnpm test src/lib/ai/__tests__/providers.test.ts

# 4. API route accessible (with dev servers running)
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello"}]}'
# Should return 401 if not authenticated (expected behavior)
```

## Success Criteria

- [ ] AI SDK packages installed (ai@5.x, ai-fallback@1.x)
- [ ] Provider configuration exports `chatModel` with 5 fallback models
- [ ] System prompt defined for Phase 26 foundation
- [ ] API route `/api/chat` exists and requires authentication
- [ ] Tests pass for provider configuration
- [ ] TypeScript compiles without errors

## Output

```
v2/src/lib/ai/
├── providers.ts       # Multi-provider fallback configuration
├── system-prompt.ts   # Base system prompt
├── index.ts           # Module exports
└── __tests__/
    └── providers.test.ts

v2/src/app/api/chat/
└── route.ts           # Streaming API route
```

## Notes

- API keys must be added to `.env.local` (not committed)
- Provider rate limits: Gemini ~10 RPM, Groq ~30 RPM, OpenRouter ~20 RPM
- Fallback triggers automatically on 429/5xx errors
- `maxDuration = 30` allows streaming up to 30 seconds

---

**Next Plan:** 26-02 (Convex Conversations Layer)
