---
phase: 10-final-validation
plan: 02
type: execute
---

<objective>
Add tests for critical 0% coverage services to prevent regressions.

Purpose: Current test coverage is 22% with major gaps in chatbot pipeline (llm_service, conversation_service) and background services (scheduler, notifications). Adding focused tests for the highest-risk untested code paths.
Output: New test files covering critical services, improving coverage from 22% toward 50%+ for core functionality.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Concern addressed:** L1 (Test Coverage Below Target) from COMPREHENSIVE_CONCERNS_SPEC.md

**Critical 0% coverage services (from exploration):**
- llm_service.py (703 lines) - CHATBOT ENGINE, handles all LLM provider calls
- conversation_service.py (151 lines) - conversation state management
- scheduler_service.py (151 lines) - deadline reminders
- notification_service.py (19% coverage, 296 lines) - push notifications
- logger.py (59 lines) - logging infrastructure

**Existing test infrastructure:**
- pytest with pytest-asyncio, pytest-cov
- conftest.py with fixtures (test client, mock user)
- 14 test files, 227 tests passing
- Test database fixtures for case/user data

**Priority order for this plan:**
1. llm_service.py - Most critical, handles all AI functionality
2. logger.py - Infrastructure used by all other services (quick win)
3. scheduler_service.py - Background job reliability

**What NOT to test (external API mocking complexity):**
- Actual LLM provider API calls (mock responses instead)
- Actual email sending via Resend
- Actual Google Calendar API calls
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add tests for logger utility</name>
  <files>backend/tests/test_logger.py</files>
  <exploration>
    Use Explore agent to understand the logger.py implementation:
    - What functions/methods does it expose?
    - How is it used across the codebase?
    - What are the configurable log levels?
  </exploration>
  <action>
Create backend/tests/test_logger.py with tests for:

1. Logger initialization with different log levels
2. Log message formatting
3. Environment-aware behavior (DEBUG vs production)
4. Structured logging output format

Example test structure:
```python
import pytest
from app.services.logger import get_logger, setup_logging

class TestLogger:
    def test_get_logger_returns_logger_instance(self):
        logger = get_logger(__name__)
        assert logger is not None
        assert hasattr(logger, 'info')
        assert hasattr(logger, 'error')
        assert hasattr(logger, 'debug')

    def test_logger_respects_log_level(self, caplog):
        # Test that debug messages are suppressed at INFO level
        pass

    def test_logger_includes_module_name(self, caplog):
        # Test that log messages include the module name
        pass
```

Focus on quick, reliable tests that verify the logging infrastructure works correctly.
  </action>
  <verify>pytest backend/tests/test_logger.py -v passes all tests</verify>
  <done>Logger utility has test coverage, infrastructure verified working</done>
</task>

<task type="auto">
  <name>Task 2: Add tests for LLM service with mocked providers</name>
  <files>backend/tests/test_llm_service.py</files>
  <exploration>
    Use Explore agent to understand llm_service.py:
    - What are the main public methods?
    - How does provider fallback work?
    - What response format is expected?
    - How are errors handled?
  </exploration>
  <action>
Create backend/tests/test_llm_service.py with mocked LLM provider tests:

1. Test provider selection logic (which provider gets called when)
2. Test fallback behavior when primary provider fails
3. Test response parsing from different providers
4. Test error handling for API failures
5. Test rate limiting / token counting if applicable

Use pytest-mock or unittest.mock to mock external API calls:
```python
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
from app.services.llm_service import LLMService

@pytest.fixture
def llm_service():
    return LLMService()

class TestLLMService:
    @pytest.mark.asyncio
    async def test_generate_response_with_valid_provider(self, llm_service):
        with patch.object(llm_service, '_call_provider') as mock_call:
            mock_call.return_value = {"content": "Test response"}
            result = await llm_service.generate_response("Hello", [])
            assert result is not None

    @pytest.mark.asyncio
    async def test_provider_fallback_on_failure(self, llm_service):
        # Test that secondary provider is tried when primary fails
        pass

    @pytest.mark.asyncio
    async def test_handles_empty_response(self, llm_service):
        # Test graceful handling of empty/null responses
        pass

    @pytest.mark.asyncio
    async def test_handles_api_error(self, llm_service):
        # Test error handling for HTTP errors
        pass
```

Key: Mock the actual HTTP calls, not the service methods. Test the service logic, not the providers.
  </action>
  <verify>pytest backend/tests/test_llm_service.py -v passes all tests</verify>
  <done>LLM service has test coverage for core logic with mocked providers</done>
</task>

<task type="auto">
  <name>Task 3: Add tests for scheduler service</name>
  <files>backend/tests/test_scheduler_service.py</files>
  <exploration>
    Use Explore agent to understand scheduler_service.py:
    - What scheduling functions are exposed?
    - How are deadlines detected and processed?
    - What triggers reminder notifications?
    - How is the scheduler started/stopped?
  </exploration>
  <action>
Create backend/tests/test_scheduler_service.py:

1. Test deadline detection logic (which cases need reminders)
2. Test reminder timing calculation
3. Test notification triggering (mock the notification service)
4. Test scheduler start/stop lifecycle

```python
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
from datetime import datetime, timedelta
from app.services.scheduler_service import SchedulerService

@pytest.fixture
def scheduler_service():
    return SchedulerService()

class TestSchedulerService:
    def test_identifies_upcoming_deadlines(self, scheduler_service):
        # Test that cases with deadlines in reminder window are identified
        pass

    def test_calculates_reminder_timing(self, scheduler_service):
        # Test reminder_days_before calculation
        pass

    @pytest.mark.asyncio
    async def test_triggers_notification_for_deadline(self, scheduler_service):
        # Mock notification_service, verify it gets called
        pass

    def test_does_not_remind_for_past_deadlines(self, scheduler_service):
        # Edge case: deadline already passed
        pass

    def test_respects_user_preferences(self, scheduler_service):
        # Test that user's reminder_days_before preference is used
        pass
```

Focus on the business logic (deadline detection, timing), mock the external actions (notifications, database queries where possible).
  </action>
  <verify>pytest backend/tests/test_scheduler_service.py -v passes all tests</verify>
  <done>Scheduler service has test coverage for deadline reminder logic</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] pytest backend/tests/test_logger.py -v passes
- [ ] pytest backend/tests/test_llm_service.py -v passes
- [ ] pytest backend/tests/test_scheduler_service.py -v passes
- [ ] Full test suite still passes: pytest backend/tests/ -v
- [ ] Coverage improved: pytest --cov=app --cov-report=term-missing backend/tests/
</verification>

<success_criteria>
- 3 new test files created for previously untested services
- All new tests pass
- No regressions in existing tests
- Coverage improved from 22% baseline (target: 30%+ for tested services)
- Critical LLM service path has test coverage
</success_criteria>

<output>
After completion, create `.planning/phases/10-final-validation/10-02-SUMMARY.md`
</output>
