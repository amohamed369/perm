# Phase 32: Data Migration + Go-Live - Research

**Researched:** 2026-01-12
**Status:** Complete

---

## Executive Summary

This research covers the migration from Supabase (PostgreSQL) to Convex for the PERM Tracker application. Key findings:

1. **Export Strategy:** Use Supabase CLI with three-file approach (roles.sql, schema.sql, data.sql), then convert to JSONLines for Convex import
2. **Import Strategy:** Use Convex CLI `npx convex import` with JSONLines format (no size limits)
3. **ID Mapping:** Store legacy UUIDs, use indexed lookups, migrate references in batches of 100
4. **Migration Pattern:** Maintenance window approach (15-30 min) - appropriate for small app scale
5. **User Matching:** Email-based matching (both systems use email as identifier)

---

## 1. Supabase Data Export

### Recommended: Three-File Backup Strategy

```bash
# 1. Export roles (not needed for Convex, but good backup)
supabase db dump --db-url $SUPABASE_URL -f roles.sql --role-only

# 2. Export schema (reference only - Convex has its own schema)
supabase db dump --db-url $SUPABASE_URL -f schema.sql

# 3. Export data using COPY for performance
supabase db dump --db-url $SUPABASE_URL -f data.sql --use-copy --data-only
```

### Convert to JSONLines for Convex

Since Convex imports JSONLines, export each table to JSON:

```bash
# Connect to Supabase via psql and export each table
\copy (SELECT row_to_json(t) FROM users t) TO 'users.jsonl'
\copy (SELECT row_to_json(t) FROM cases t) TO 'cases.jsonl'
\copy (SELECT row_to_json(t) FROM user_settings t) TO 'user_settings.jsonl'
\copy (SELECT row_to_json(t) FROM user_preferences t) TO 'user_preferences.jsonl'
\copy (SELECT row_to_json(t) FROM notifications t) TO 'notifications.jsonl'
```

### Key Considerations

| Aspect | Decision |
|--------|----------|
| Export format | JSONLines (one JSON object per line) |
| Size limit | None for JSONLines (vs 8MiB for JSON) |
| Relationships | Preserved in data, mapped during import |
| Auth tables | Skip `auth.*` - users will re-authenticate in v2 |

---

## 2. Convex Bulk Import

### Import Commands

```bash
# Import each table
npx convex import --table users users.jsonl
npx convex import --table cases cases.jsonl
npx convex import --table userProfiles userProfiles.jsonl

# Or import from full backup
npx convex import backup.zip
```

### File Format Requirements

| Format | Size Limit | Best For |
|--------|------------|----------|
| **JSONLines** | Unlimited | **RECOMMENDED** - one object per line |
| CSV | Unlimited | Simple data (strings/numbers only) |
| JSON | 8 MiB | Small datasets only |

### Convert JSON to JSONLines

```bash
# If you have a JSON array, convert with jq
jq -c '.[]' data.json > data.jsonl
```

### Rate Limits & Performance

- No specific batch size limits in Convex import
- Import uses database bandwidth (monitor via usage dashboard)
- ZIP imports are atomic (all or nothing)

---

## 3. ID Mapping Strategy (UUID → Convex Id)

### Five-Phase Approach

**Phase 1: Import with Legacy IDs**
```typescript
// During import, add legacyId field
{
  _id: "convex_generated",  // Auto-generated by Convex
  legacyId: "uuid-from-postgres",  // Preserved for reference lookup
  // ... other fields
}
```

**Phase 2: Add Index for Legacy Lookup**
```typescript
// In schema.ts
users: defineTable({...})
  .index("by_legacy_id", ["legacyId"])

cases: defineTable({...})
  .index("by_legacy_id", ["legacyId"])
```

**Phase 3: Migration Script to Resolve References**
```typescript
import { Migrations } from "@convex-dev/migrations";

export const migrateCaseUserIds = migrations.define({
  table: "cases",
  batchSize: 100,  // Default batch size
  migrateOne: async (ctx, doc) => {
    if (doc.legacyUserId) {
      const user = await ctx.db
        .query("users")
        .withIndex("by_legacy_id", q => q.eq("legacyId", doc.legacyUserId))
        .first();

      if (user) {
        return { userId: user._id, legacyUserId: undefined };
      }
    }
    return {};
  },
});
```

**Phase 4: Run Migration**
```bash
npx convex run migrations:migrateCaseUserIds
```

**Phase 5: Remove Legacy Fields (Post-Verification)**
```typescript
// After verification, clean up legacy fields
export const cleanupLegacyIds = migrations.define({
  table: "cases",
  migrateOne: () => ({ legacyId: undefined, legacyUserId: undefined }),
});
```

### Batch Size Recommendations

| Document Size | Batch Size | Notes |
|--------------|------------|-------|
| Small (<1KB) | 100 | Default, works well |
| Medium (1-10KB) | 50 | Reduce for larger docs |
| Large (>10KB) | 10-25 | Avoid transaction limits |

### Helper for Dual ID Lookup (Transition Period)

```typescript
async function getByEitherId(ctx, table, id) {
  // Try Convex ID first
  const convexId = ctx.db.normalizeId(table, id);
  if (convexId) {
    return ctx.db.get(convexId);
  }

  // Fallback to legacy UUID lookup
  return ctx.db.query(table)
    .withIndex("by_legacy_id", q => q.eq("legacyId", id))
    .first();
}
```

---

## 4. Migration Pattern: Maintenance Window

### Why Maintenance Window (Not Zero-Downtime)

| Factor | Decision |
|--------|----------|
| User count | Small (<1000 users) |
| Downtime tolerance | Acceptable (15-30 min) |
| Complexity | Zero-downtime requires dual-write, feature flags, cohorts |
| Risk | Lower with clean cutover |

**Recommendation:** 15-30 minute maintenance window is appropriate for PERM Tracker's scale.

### Migration Sequence

```
T-30 days: Email announcement
T-7 days:  Reminder + exact timeline
T-1 day:   Final reminder
T-0:       Maintenance window begins
  |
  +-- 1. Set v1 to read-only mode (10 sec)
  +-- 2. Export from Supabase (2 min)
  +-- 3. Transform data (5 min)
  +-- 4. Import to Convex production (5 min)
  +-- 5. Run ID migrations (5 min)
  +-- 6. Verify 229 features (5 min)
  +-- 7. DNS cutover to Vercel (instant)
  +-- 8. Smoke test (2 min)
  |
T+30 min: Migration complete, users notified
T+24 hr:  Run v1 calendar cleanup
T+7 days: Decommission v1 services
```

### User Communication Template

```
Subject: PERM Tracker - Brief Maintenance Window on [DATE]

Hi [Name],

We're upgrading PERM Tracker to a faster, more reliable platform.

**What's happening:**
- Scheduled maintenance: [DATE] at 11 PM - 12 AM EST (30 min max)
- During this time, the app will be unavailable

**What you need to do:**
- Nothing! All your cases and data will be preserved
- Sign in with Google as usual after maintenance

**What's new:**
- Faster real-time updates
- Improved mobile experience
- Enhanced chatbot capabilities

Questions? Reply to this email.

Thank you for using PERM Tracker!
```

### Rollback Criteria

**Immediate Rollback (within 10 min):**
- Any data loss detected
- Critical features not working
- Authentication failing

**Continue with Fixes:**
- Minor UI issues
- Non-critical features degraded
- Performance within acceptable range

### Rollback Procedure

```bash
# 1. Point DNS back to v1 Vercel
# 2. Restore Supabase from backup (if needed)
# 3. Announce rollback to users
# 4. Schedule post-mortem
```

---

## 5. Email-Based User Matching

### Strategy

Both v1 (Supabase) and v2 (Convex) use email as the primary identifier:

```typescript
// Match users by email (case-insensitive)
async function matchUser(ctx, supabaseEmail) {
  const normalizedEmail = supabaseEmail.toLowerCase().trim();

  return ctx.db.query("users")
    .withIndex("email", q => q.eq("email", normalizedEmail))
    .first();
}
```

### Edge Cases

| Edge Case | Handling |
|-----------|----------|
| Email not in v2 | Create userProfile after first login |
| Duplicate emails | Run pre-migration audit, manual resolution |
| Email changes | Track in legacyEmail field |
| Unverified emails | Flag for review |

### Pre-Migration Email Audit

```sql
-- Run on Supabase before migration
SELECT
  COUNT(*) as total_users,
  COUNT(DISTINCT LOWER(email)) as unique_emails,
  COUNT(*) - COUNT(email) as null_emails
FROM auth.users;

-- Check for duplicates
SELECT LOWER(email), COUNT(*)
FROM auth.users
WHERE email IS NOT NULL
GROUP BY LOWER(email)
HAVING COUNT(*) > 1;
```

### User Re-Authentication Flow

1. User visits v2 at permtracker.app
2. User clicks "Sign in with Google"
3. Convex Auth creates/matches user by email
4. userProfile is created (or matched via migration data)
5. User sees their cases (migrated with their email-matched userId)

**Key insight:** Users don't need to "migrate" - they just log in and their data is there.

---

## 6. Data Transformation Rules

Already documented in `.planning/V2_FIELD_MAPPINGS.md`. Key transformations:

| v1 (PostgreSQL) | v2 (Convex) | Transformation |
|-----------------|-------------|----------------|
| `UUID` | `Id<"table">` | Store as legacyId, generate new |
| `TIMESTAMP` | `number` | Unix milliseconds |
| `DECIMAL(10,2)` | `v.int64()` | Multiply by 100 (cents) |
| `snake_case` | `camelCase` | Field name conversion |
| `DATE` | `string` | Keep as ISO YYYY-MM-DD |
| `JSONB` | `v.array()`/`v.object()` | Parse and type |

### Transformation Script Structure

```typescript
// scripts/transform.ts
import type { Doc } from "../convex/_generated/dataModel";

function transformCase(pgCase: PostgresCase): Partial<Doc<"cases">> {
  return {
    // Generate new Convex ID automatically during import
    legacyId: pgCase.id,  // Keep for reference

    // Map user reference
    legacyUserId: pgCase.user_id,  // Resolve after import

    // Field name conversions
    employerName: pgCase.employer_name,
    positionTitle: pgCase.position_title,
    beneficiaryIdentifier: pgCase.beneficiary_identifier,

    // Date transformations (keep as ISO strings)
    pwdFilingDate: pgCase.pwd_filing_date,
    pwdDeterminationDate: pgCase.pwd_determination_date,

    // Timestamp conversions
    createdAt: new Date(pgCase.created_at).getTime(),
    updatedAt: new Date(pgCase.updated_at).getTime(),

    // Money conversions (cents)
    pwdWageAmount: pgCase.pwd_wage_amount
      ? Math.round(parseFloat(pgCase.pwd_wage_amount) * 100)
      : undefined,

    // Boolean defaults
    isProfessionalOccupation: pgCase.is_professional_occupation ?? false,
    isFavorite: pgCase.is_favorite ?? false,
    calendarSyncEnabled: pgCase.calendar_sync_enabled ?? true,

    // Status mappings (already compatible)
    caseStatus: pgCase.case_status,
    progressStatus: pgCase.progress_status,

    // Arrays
    tags: pgCase.tags ?? [],
    additionalRecruitmentMethods: transformRecruitmentMethods(
      pgCase.additional_recruitment_methods
    ),
  };
}
```

---

## 7. Verification Strategy

### 229-Feature Verification Approach

**Automated Checks (Pre-Go-Live):**

```typescript
// scripts/verify-migration.ts
async function verifyMigration() {
  const results = {
    recordCounts: await verifyRecordCounts(),
    sampleData: await verifySampleData(),
    foreignKeys: await verifyForeignKeys(),
    calculations: await verifyCalculations(),
  };

  return results;
}

async function verifyRecordCounts() {
  // Compare source vs target counts
  const sourceCounts = await getSupabaseCounts();
  const targetCounts = await getConvexCounts();

  return {
    users: sourceCounts.users === targetCounts.users,
    cases: sourceCounts.cases === targetCounts.cases,
    // ...
  };
}

async function verifySampleData() {
  // Verify 10% random sample matches
  const sample = await getRandomCaseSample(0.1);
  for (const id of sample) {
    const source = await getSupabaseCase(id);
    const target = await getConvexCaseByLegacyId(id);
    assertFieldsMatch(source, target);
  }
}
```

**Manual Smoke Test Checklist:**

- [ ] Login with test Google account
- [ ] Dashboard loads with correct case count
- [ ] Case detail shows all fields correctly
- [ ] Date calculations are accurate
- [ ] Add new case works
- [ ] Edit case works
- [ ] Notifications appear
- [ ] Calendar sync toggle works
- [ ] Chatbot responds
- [ ] Export CSV works

---

## 8. Calendar Cleanup (v1)

### The Orphaned Events Problem

v1 has a bug: when calendar sync is disabled or a case is deleted, events remain in Google Calendar.

**Cleanup Script (Run T+24hr):**

```python
# backend/scripts/cleanup_v1_calendar.py
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build

def cleanup_all_v1_events():
    """Delete ALL PERM Tracker events from Google Calendar for all users."""

    users = get_all_users_with_calendar_tokens()

    for user in users:
        try:
            creds = Credentials(
                token=decrypt(user.google_access_token),
                refresh_token=decrypt(user.google_refresh_token),
            )
            service = build('calendar', 'v3', credentials=creds)

            # Find and delete PERM Tracker events
            events = service.events().list(
                calendarId='primary',
                q='PERM Tracker'  # All v1 events have this in description
            ).execute()

            for event in events.get('items', []):
                service.events().delete(
                    calendarId='primary',
                    eventId=event['id']
                ).execute()

            log(f"Cleaned {len(events['items'])} events for {user.email}")

        except Exception as e:
            log(f"Failed for {user.email}: {e}")
```

### v2 Calendar Fixes Needed (Pre-Launch)

1. **Auto-cleanup on preference toggle:**
   - When user turns off `calendarSyncPwd`, delete all PWD events
   - Same for each deadline type toggle

2. **"Clear All Events" button:**
   - Add to settings disconnect flow
   - Deletes all synced events before disconnecting

3. **RFI/RFE multi-entry tracking:**
   - Store array of event IDs (currently single ID)
   - Delete all when case/entry deleted

---

## 9. Decommissioning v1

### Decommission Timeline

| Time | Action |
|------|--------|
| T+0 | Migration complete, v2 live |
| T+24hr | Run v1 calendar cleanup script |
| T+7 days | Monitor for issues, verify no v1 access |
| T+14 days | Decommission Render backend |
| T+30 days | Decommission Supabase (keep backup) |
| T+30 days | Remove v1 code from repo |

### Decommission Checklist

- [ ] v2 stable for 7+ days
- [ ] No error reports referencing v1
- [ ] v1 calendar events cleaned up
- [ ] DNS fully propagated (check with multiple resolvers)
- [ ] Supabase data backed up locally
- [ ] Render service scaled to 0
- [ ] Supabase project paused
- [ ] v1 code removed from repo (keep v2/ only)
- [ ] Repository cleaned (move v2/ contents to root)

---

## 10. Migration Script Architecture

### Directory Structure

```
backend/scripts/migration/
├── 01_export_supabase.sh      # Export all tables to JSONLines
├── 02_transform_data.ts       # Apply field transformations
├── 03_import_convex.sh        # Import to production Convex
├── 04_migrate_ids.ts          # Resolve UUID → Convex Id references
├── 05_verify_migration.ts     # Automated verification
├── 06_cleanup_calendar.py     # Delete v1 Google Calendar events
└── README.md                  # Step-by-step instructions
```

### Prerequisites

- Supabase CLI installed and authenticated
- Convex CLI authenticated to production deployment
- Node.js 18+
- Python 3.9+ (for calendar cleanup)
- Google API credentials (for calendar cleanup)

---

## Key Decisions Summary

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Export format | JSONLines | No size limit, Convex-native |
| Migration approach | Maintenance window | Simple, appropriate for scale |
| Window duration | 30 min max | Conservative estimate |
| ID mapping | Legacy field + index | Clean migration path |
| User matching | Email-based | Both systems use email |
| Batch size | 100 (default) | Convex migrations component default |
| Calendar cleanup | T+24hr | After v2 verified stable |
| v1 decommission | T+14 days | Allow buffer for issues |

---

## What NOT to Hand-Roll

| Don't Build | Use Instead |
|-------------|-------------|
| Custom batch importer | `npx convex import` |
| Migration framework | `@convex-dev/migrations` component |
| JSON → JSONLines converter | `jq -c '.[]'` |
| Calendar event deletion | Google Calendar API directly |
| UUID generation | Convex auto-generates IDs |

---

## Sources

### Official Documentation
- [Convex Data Import](https://docs.convex.dev/database/import-export/import)
- [Convex Migrations Component](https://www.convex.dev/components/migrations)
- [Supabase CLI Database Dump](https://supabase.com/docs/reference/cli/supabase-db-dump)
- [Migrate Data from Postgres to Convex](https://stack.convex.dev/migrate-data-postgres-to-convex)

### Best Practices
- [Stateful Online Migrations with Convex](https://stack.convex.dev/migrating-data-with-mutations)
- [Zero-Downtime Database Migration Guide](https://dev.to/ari-ghosh/zero-downtime-database-migration-the-definitive-guide-5672)

---

*Research complete. Ready for planning.*
