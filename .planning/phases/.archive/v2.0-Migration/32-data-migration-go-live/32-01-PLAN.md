---
phase: 32-data-migration-go-live
plan: 01
type: execute
---

<objective>
Create migration scripts and infrastructure for Supabase → Convex data migration.

Purpose: Build the tooling needed to export v1 data, transform it to v2 format, and import into Convex.
Output: Complete migration scripts in `v2/scripts/migration/` ready to execute.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/V2_FIELD_MAPPINGS.md
@.planning/phases/32-data-migration-go-live/32-RESEARCH.md
@.planning/phases/32-data-migration-go-live/32-CONTEXT.md

**Prior decisions affecting this plan:**
- JSONLines format for import (no size limit vs 8MiB for JSON)
- Maintenance window approach (15-30 min)
- Email-based user matching (both v1 and v2 use email as identifier)
- Legacy ID fields + indexes for reference resolution
- Batch size of 100 for migrations

**Tables to migrate:**
- users → userProfiles (v2 uses separate users table from Convex Auth)
- cases → cases
- user_settings → userProfiles (consolidated)
- user_preferences → userProfiles (consolidated)
- notifications → NOT migrated (fresh start)
- conversations → NOT migrated (fresh start)

**TEST WORKTREE CONTEXT:**
- This is `perm-tracker-test/` worktree on branch `test`
- Migration scripts will be developed and tested here
- Production migration will be executed from main branch
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create migration directory structure and export script</name>
  <files>v2/scripts/migration/01_export_supabase.sh, v2/scripts/migration/README.md</files>
  <exploration>
    Use Explore agent to check existing backend/scripts/ patterns.
    Look at how psql/supabase CLI is used in the v1 codebase.
  </exploration>
  <action>
Create `v2/scripts/migration/` directory with:

1. `README.md` - Step-by-step migration guide:
   - Prerequisites (Supabase CLI, Node.js, Convex CLI)
   - Environment variables needed
   - Execution order
   - Verification steps
   - Rollback procedure

2. `01_export_supabase.sh` - Export script that:
   - Connects to Supabase via connection string from env
   - Exports tables to JSONLines using psql `\copy (SELECT row_to_json(t) FROM table t)`
   - Tables to export: users (from auth.users), cases, user_settings, user_preferences
   - Outputs to `v2/scripts/migration/data/` directory
   - Validates exported files exist and have content

Script should be idempotent (can run multiple times safely).
  </action>
  <verify>chmod +x script and verify help text displays with --help flag</verify>
  <done>Export script created, documented, and executable</done>
</task>

<task type="auto">
  <name>Task 2: Create data transformation script</name>
  <files>v2/scripts/migration/02_transform_data.ts, v2/scripts/migration/types.ts</files>
  <exploration>
    Review V2_FIELD_MAPPINGS.md for complete field transformations.
    Check v2/convex/schema.ts for target field types.
  </exploration>
  <action>
Create TypeScript transformation script:

1. `types.ts` - Type definitions:
   - PostgresUser, PostgresCase, PostgresUserSettings, PostgresUserPreferences
   - ConvexUserProfile, ConvexCase
   - TransformationResult

2. `02_transform_data.ts` - Transformation logic:
   - Read JSONLines files from `data/` directory
   - Transform each record per V2_FIELD_MAPPINGS.md:
     - snake_case → camelCase field names
     - TIMESTAMP → Unix milliseconds
     - DECIMAL(10,2) → cents (multiply by 100)
     - DATE → keep as ISO YYYY-MM-DD string
     - Add `legacyId` field with original UUID
     - Add `legacyUserId` field (for cases) with original user UUID
   - Consolidate user_settings + user_preferences into userProfiles
   - Handle null values and defaults per schema
   - Write transformed data to `data/transformed/` as JSONLines
   - Log transformation stats (records processed, errors)

Use ts-node or tsx to run. Handle:
- Empty arrays for tags, additionalRecruitmentMethods
- Default boolean values per V2_FIELD_MAPPINGS.md
- RFI/RFE entries transformation (array of objects)
  </action>
  <verify>npx tsx 02_transform_data.ts --dry-run shows transformation preview</verify>
  <done>Transformation script handles all field mappings correctly</done>
</task>

<task type="auto">
  <name>Task 3: Create Convex import and ID migration scripts</name>
  <files>v2/scripts/migration/03_import_convex.sh, v2/convex/migrations.ts, v2/convex/convex.config.ts</files>
  <exploration>
    Check current convex/convex.config.ts for migrations component setup.
    Review @convex-dev/migrations documentation patterns.
  </exploration>
  <action>
Create import infrastructure:

1. `03_import_convex.sh` - Import script that:
   - Uses `npx convex import --table <name> <file>.jsonl` for each table
   - Imports userProfiles first (users come from Convex Auth)
   - Then imports cases
   - Validates record counts match source

2. Update `convex/convex.config.ts` to include migrations component:
   ```typescript
   import { defineApp } from 'convex/server';
   import migrations from '@convex-dev/migrations/convex.config';

   const app = defineApp();
   app.use(migrations);
   export default app;
   ```

3. `convex/migrations.ts` - ID resolution migrations using @convex-dev/migrations:
   - Migration to resolve legacyUserId → userId on cases table
   - Uses index on legacyId for efficient lookup
   - Batch size of 100
   - Clears legacy fields after verification

4. Add legacyId indexes to schema.ts if not present:
   - cases: .index("by_legacy_id", ["legacyId"])
   - userProfiles: .index("by_legacy_id", ["legacyId"])

Note: Don't add legacyId/legacyUserId fields to schema yet - they'll be added during migration execution (schema allows extra fields).
  </action>
  <verify>npx convex dev compiles migrations.ts without errors</verify>
  <done>Import script and ID migration infrastructure ready</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All migration scripts are executable (chmod +x)
- [ ] README.md documents complete workflow
- [ ] TypeScript transformation compiles without errors
- [ ] Convex migrations component configured
- [ ] Dry-run of transform script shows correct field mappings
</verification>

<success_criteria>
- Migration scripts directory created with 3 executable scripts
- README.md provides clear step-by-step guide
- Transformation handles all 156+ fields per V2_FIELD_MAPPINGS.md
- Convex migrations component ready for ID resolution
- Scripts tested in dry-run mode
</success_criteria>

<output>
After completion, create `.planning/phases/32-data-migration-go-live/32-01-SUMMARY.md`
</output>
