---
phase: 27-knowledge-layer
plan: 05
type: execute
domain: context-management
---

<objective>
Implement context management for long conversations: summarization and tool result caching.

Purpose: Optimize token usage for long conversations and reduce redundant tool calls. Industry standard patterns: rolling summary (12+ messages triggers compression, keep 6 recent), TTL-based tool caching (5m cases, 15m web, 24h knowledge).

Output:
- Conversation summarization (12+ messages → 6 recent + summary)
- Tool result caching with TTL-based expiration
- Context optimization reducing token usage for long chats
- Human verification of context management features
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-knowledge-layer/27-RESEARCH.md
@.planning/phases/27-knowledge-layer/27-CONTEXT.md
@.planning/phases/27-knowledge-layer/27-04-SUMMARY.md
@v2/src/app/api/chat/route.ts
@v2/convex/schema.ts
@v2/convex/conversationMessages.ts

**Prior decisions affecting this phase:**
- Phase 26: Chat API route with probe-before-stream fallback established
- Phase 26: Message schema includes `toolCalls` array with status/result
- Phase 27-04: Native AI SDK tool calling integrated with streamText

**Context Management (Industry Standard Research):**
- Summarize at 3,000-4,000 tokens OR 10-15 messages
- Rolling summary pattern: keep last 5-8 raw + compressed history
- Tool result caching: session-scoped for case data, 15m TTL for web search
- Token budget: ~8K total (system ~800, summary ~500, recent ~2K, tools ~3K, buffer ~1.7K)
- Store UI messages separately from LLM context (two-tier display pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement conversation summarization</name>
  <files>v2/convex/schema.ts, v2/convex/conversationSummary.ts, v2/src/app/api/chat/route.ts, v2/src/lib/ai/summarize.ts</files>
  <exploration>
    Review conversation schema to understand existing metadata field.
    Check current message fetching in chat API route.
  </exploration>
  <action>
    1. Update schema.ts to add summary field to conversations:

    ```typescript
    // In conversations table
    conversations: defineTable({
      // ... existing fields ...
      summary: v.optional(v.object({
        content: v.string(),         // Compressed history text
        tokenCount: v.number(),      // Tokens in summary
        messageCountAtSummary: v.number(), // Messages when summarized
        lastSummarizedAt: v.number(), // Timestamp
      })),
    })
    ```

    2. Create convex/conversationSummary.ts:

    ```typescript
    import { v } from "convex/values";
    import { internalAction, internalMutation, internalQuery } from "./_generated/server";
    import { internal } from "./_generated/api";
    import { Id } from "./_generated/dataModel";

    // Constants
    const SUMMARY_TRIGGER_MESSAGE_COUNT = 12;  // Start summarizing after 12 messages
    const RECENT_MESSAGES_TO_KEEP = 6;         // Keep last 6 raw
    const SUMMARY_MAX_TOKENS = 800;            // Target summary size

    /**
     * Check if conversation needs summarization
     */
    export const needsSummarization = internalQuery({
      args: { conversationId: v.id("conversations") },
      handler: async (ctx, { conversationId }) => {
        const messages = await ctx.db
          .query("conversationMessages")
          .withIndex("by_conversation_id", q => q.eq("conversationId", conversationId))
          .collect();

        const conversation = await ctx.db.get(conversationId);
        const lastSummarizedCount = conversation?.summary?.messageCountAtSummary ?? 0;
        const newMessagesSinceSummary = messages.length - lastSummarizedCount;

        return {
          needed: messages.length >= SUMMARY_TRIGGER_MESSAGE_COUNT && newMessagesSinceSummary >= 6,
          totalMessages: messages.length,
          recentCount: RECENT_MESSAGES_TO_KEEP,
          messagesToSummarize: messages.length - RECENT_MESSAGES_TO_KEEP,
        };
      },
    });

    /**
     * Get messages for LLM context (recent raw + summary)
     */
    export const getContextMessages = internalQuery({
      args: { conversationId: v.id("conversations") },
      handler: async (ctx, { conversationId }) => {
        const conversation = await ctx.db.get(conversationId);
        const messages = await ctx.db
          .query("conversationMessages")
          .withIndex("by_conversation_id", q => q.eq("conversationId", conversationId))
          .order("asc")
          .collect();

        // Get last N messages raw
        const recentMessages = messages.slice(-RECENT_MESSAGES_TO_KEEP);

        return {
          summary: conversation?.summary?.content ?? null,
          recentMessages: recentMessages.map(m => ({
            role: m.role,
            content: m.content,
          })),
          totalMessageCount: messages.length,
        };
      },
    });

    /**
     * Get messages to summarize (older messages not in recent window)
     */
    export const getMessagesToSummarize = internalQuery({
      args: { conversationId: v.id("conversations") },
      handler: async (ctx, { conversationId }) => {
        const conversation = await ctx.db.get(conversationId);
        const messages = await ctx.db
          .query("conversationMessages")
          .withIndex("by_conversation_id", q => q.eq("conversationId", conversationId))
          .order("asc")
          .collect();

        // Messages to summarize = all except last RECENT_MESSAGES_TO_KEEP
        const toSummarize = messages.slice(0, -RECENT_MESSAGES_TO_KEEP);

        return {
          existingSummary: conversation?.summary?.content ?? null,
          messages: toSummarize.map(m => ({
            role: m.role,
            content: m.content,
          })),
        };
      },
    });

    /**
     * Save conversation summary
     */
    export const saveSummary = internalMutation({
      args: {
        conversationId: v.id("conversations"),
        summary: v.string(),
        tokenCount: v.number(),
        messageCount: v.number(),
      },
      handler: async (ctx, { conversationId, summary, tokenCount, messageCount }) => {
        await ctx.db.patch(conversationId, {
          summary: {
            content: summary,
            tokenCount,
            messageCountAtSummary: messageCount,
            lastSummarizedAt: Date.now(),
          },
        });
      },
    });

    /**
     * Summarization prompt for LLM
     */
    export const SUMMARIZATION_PROMPT = `You are summarizing a conversation between a user and a PERM immigration case assistant.

Create a concise summary that preserves:
- Key facts discussed (case names, deadlines, dates)
- Decisions made or advice given
- Important context the assistant needs to remember
- Any pending questions or unresolved topics

Keep the summary under 400 words. Use bullet points for clarity.
Format: Start with "Previous conversation summary:" then the bullet points.`;
    ```

    3. Create src/lib/ai/summarize.ts for async summarization:

    ```typescript
    import { generateText } from 'ai';
    import { fetchQuery, fetchMutation } from 'convex/nextjs';
    import { internal } from '@/convex/_generated/api';
    import { google } from '@ai-sdk/google';

    const SUMMARIZATION_PROMPT = `You are summarizing a conversation between a user and a PERM immigration case assistant.

Create a concise summary that preserves:
- Key facts discussed (case names, deadlines, dates)
- Decisions made or advice given
- Important context the assistant needs to remember
- Any pending questions or unresolved topics

Keep the summary under 400 words. Use bullet points for clarity.
Format: Start with "Previous conversation summary:" then the bullet points.`;

    export async function summarizeConversation(conversationId: string, token: string) {
      // Get messages to summarize
      const { existingSummary, messages } = await fetchQuery(
        internal.conversationSummary.getMessagesToSummarize,
        { conversationId },
        { token }
      );

      if (messages.length === 0) return;

      // Build prompt
      const conversationText = messages
        .map(m => `${m.role}: ${m.content}`)
        .join('\n\n');

      const prompt = existingSummary
        ? `${SUMMARIZATION_PROMPT}\n\nExisting summary to incorporate:\n${existingSummary}\n\nNew messages to add:\n${conversationText}`
        : `${SUMMARIZATION_PROMPT}\n\nConversation to summarize:\n${conversationText}`;

      // Generate summary (use fast model)
      const { text } = await generateText({
        model: google('gemini-2.0-flash-lite'),
        prompt,
        maxTokens: 500,
      });

      // Save summary
      await fetchMutation(
        internal.conversationSummary.saveSummary,
        {
          conversationId,
          summary: text,
          tokenCount: Math.ceil(text.length / 4),  // Rough estimate
          messageCount: messages.length,
        },
        { token }
      );

      console.log(`Summarized conversation ${conversationId}: ${messages.length} messages → ~${Math.ceil(text.length / 4)} tokens`);
    }
    ```

    4. Update chat API route to use context messages:

    ```typescript
    // In POST handler, after auth check
    import { fetchQuery } from 'convex/nextjs';
    import { internal } from '@/convex/_generated/api';
    import { summarizeConversation } from '@/lib/ai/summarize';

    const { messages: uiMessages, conversationId } = await req.json();

    // Get optimized context (summary + recent messages)
    let contextMessages = uiMessages;

    if (conversationId) {
      const { summary, recentMessages, totalMessageCount } = await fetchQuery(
        internal.conversationSummary.getContextMessages,
        { conversationId },
        { token }
      );

      if (summary && totalMessageCount > 12) {
        // Build context with summary + recent
        contextMessages = [
          { role: 'system', content: summary },  // Inject summary as system context
          ...recentMessages,
        ];
        console.log(`Using summarized context: ${totalMessageCount} messages → ${recentMessages.length} recent + summary`);
      }
    }

    // Use contextMessages instead of uiMessages in streamText
    const result = streamText({
      model,
      system: SYSTEM_PROMPT,
      messages: contextMessages,  // <-- Use optimized context
      tools,
      // ...
    });

    // After streamText, check if summarization needed
    if (conversationId) {
      const { needed } = await fetchQuery(
        internal.conversationSummary.needsSummarization,
        { conversationId },
        { token }
      );

      if (needed) {
        // Trigger async summarization (don't block response)
        summarizeConversation(conversationId, token).catch(console.error);
      }
    }
    ```

    AVOID: Don't summarize on every message - only trigger after threshold crossed.
  </action>
  <verify>
    - Schema deploys with new summary field
    - Conversations work normally without summary
    - After 12+ messages, summary is generated
    - Context sent to LLM uses summary + recent (not all messages)
    - Console shows compression ratio
  </verify>
  <done>Conversation summarization reduces token usage for long chats</done>
</task>

<task type="auto">
  <name>Task 2: Implement tool result caching</name>
  <files>v2/convex/schema.ts, v2/convex/toolCache.ts, v2/src/app/api/chat/route.ts</files>
  <exploration>
    Review tool execute functions in chat route.
    Understand when caching makes sense vs fresh queries.
  </exploration>
  <action>
    1. Add toolCache table to schema.ts:

    ```typescript
    toolCache: defineTable({
      conversationId: v.id("conversations"),
      toolName: v.string(),           // "query_cases" | "search_knowledge" | "search_web"
      queryHash: v.string(),          // Hash of query params for lookup
      queryParams: v.string(),        // JSON of original params (for debugging)
      result: v.string(),             // JSON stringified result
      createdAt: v.number(),
      expiresAt: v.number(),          // TTL-based expiration
    })
      .index("by_conversation_tool_hash", ["conversationId", "toolName", "queryHash"])
      .index("by_expires", ["expiresAt"]),
    ```

    2. Create convex/toolCache.ts:

    ```typescript
    import { v } from "convex/values";
    import { mutation, query, internalMutation } from "./_generated/server";
    import { Id } from "./_generated/dataModel";

    // TTLs by tool type (in milliseconds)
    const CACHE_TTL = {
      query_cases: 5 * 60 * 1000,      // 5 minutes (case data may change)
      search_knowledge: 24 * 60 * 60 * 1000, // 24 hours (static knowledge)
      search_web: 15 * 60 * 1000,      // 15 minutes (web results fresher)
    } as const;

    /**
     * Simple hash function for cache key
     */
    function hashParams(params: Record<string, unknown>): string {
      const str = JSON.stringify(params, Object.keys(params).sort());
      let hash = 0;
      for (let i = 0; i < str.length; i++) {
        const char = str.charCodeAt(i);
        hash = ((hash << 5) - hash) + char;
        hash = hash & hash;
      }
      return Math.abs(hash).toString(36);
    }

    /**
     * Get cached result if exists and not expired
     */
    export const get = query({
      args: {
        conversationId: v.id("conversations"),
        toolName: v.string(),
        queryParams: v.any(),
      },
      handler: async (ctx, { conversationId, toolName, queryParams }) => {
        const queryHash = hashParams(queryParams);
        const now = Date.now();

        const cached = await ctx.db
          .query("toolCache")
          .withIndex("by_conversation_tool_hash", q =>
            q.eq("conversationId", conversationId)
              .eq("toolName", toolName)
              .eq("queryHash", queryHash)
          )
          .first();

        if (cached && cached.expiresAt > now) {
          return {
            hit: true,
            result: JSON.parse(cached.result),
            cachedAt: cached.createdAt,
          };
        }

        return { hit: false, result: null };
      },
    });

    /**
     * Store result in cache
     */
    export const set = mutation({
      args: {
        conversationId: v.id("conversations"),
        toolName: v.string(),
        queryParams: v.any(),
        result: v.any(),
      },
      handler: async (ctx, { conversationId, toolName, queryParams, result }) => {
        const queryHash = hashParams(queryParams);
        const now = Date.now();
        const ttl = CACHE_TTL[toolName as keyof typeof CACHE_TTL] ?? CACHE_TTL.query_cases;

        // Upsert: delete existing and insert new
        const existing = await ctx.db
          .query("toolCache")
          .withIndex("by_conversation_tool_hash", q =>
            q.eq("conversationId", conversationId)
              .eq("toolName", toolName)
              .eq("queryHash", queryHash)
          )
          .first();

        if (existing) {
          await ctx.db.delete(existing._id);
        }

        await ctx.db.insert("toolCache", {
          conversationId,
          toolName,
          queryHash,
          queryParams: JSON.stringify(queryParams),
          result: JSON.stringify(result),
          createdAt: now,
          expiresAt: now + ttl,
        });
      },
    });

    /**
     * Invalidate all case-related caches for a conversation
     * (Call this when user modifies case data)
     */
    export const invalidateCaseCaches = mutation({
      args: { conversationId: v.id("conversations") },
      handler: async (ctx, { conversationId }) => {
        const cached = await ctx.db
          .query("toolCache")
          .withIndex("by_conversation_tool_hash", q =>
            q.eq("conversationId", conversationId)
              .eq("toolName", "query_cases")
          )
          .collect();

        for (const entry of cached) {
          await ctx.db.delete(entry._id);
        }

        return { invalidated: cached.length };
      },
    });

    /**
     * Clean up expired cache entries (scheduled job)
     */
    export const cleanExpired = internalMutation({
      handler: async (ctx) => {
        const now = Date.now();
        const expired = await ctx.db
          .query("toolCache")
          .withIndex("by_expires", q => q.lt("expiresAt", now))
          .take(100);  // Batch cleanup

        for (const entry of expired) {
          await ctx.db.delete(entry._id);
        }

        return { cleaned: expired.length };
      },
    });
    ```

    3. Update tool execute functions in chat route to use cache:

    ```typescript
    import { fetchQuery, fetchMutation } from 'convex/nextjs';
    import { api } from '@/convex/_generated/api';
    import { Id } from '@/convex/_generated/dataModel';

    // Wrap tool execution with caching
    async function executeWithCache<T>(
      toolName: string,
      params: Record<string, unknown>,
      conversationId: Id<"conversations"> | null,
      execute: () => Promise<T>,
      token: string
    ): Promise<{ result: T; cached: boolean }> {
      // Skip cache if no conversation context
      if (!conversationId) {
        return { result: await execute(), cached: false };
      }

      // Check cache
      const { hit, result: cachedResult } = await fetchQuery(
        api.toolCache.get,
        { conversationId, toolName, queryParams: params },
        { token }
      );

      if (hit) {
        console.log(`Cache HIT: ${toolName}`);
        return { result: cachedResult as T, cached: true };
      }

      // Execute and cache
      console.log(`Cache MISS: ${toolName}`);
      const result = await execute();

      // Store in cache (fire-and-forget)
      fetchMutation(
        api.toolCache.set,
        { conversationId, toolName, queryParams: params, result },
        { token }
      ).catch(console.error);

      return { result, cached: false };
    }

    // In tool execute function:
    query_cases: tool({
      ...queryCasesTool,
      execute: async (params) => {
        const { result } = await executeWithCache(
          'query_cases',
          params,
          conversationId,
          () => fetchQuery(api.chatCaseData.queryCases, params, { token }),
          token
        );
        return result;
      },
    }),

    // Apply same pattern to search_knowledge and search_web
    ```

    4. Add cache stats logging:

    ```typescript
    // Track cache stats during request
    let cacheHits = 0;
    let cacheMisses = 0;

    // In executeWithCache wrapper:
    if (hit) cacheHits++;
    else cacheMisses++;

    // Log after response
    console.log(`Tool cache: ${cacheHits} hits, ${cacheMisses} misses`);
    ```

    AVOID: Don't cache errors - only successful results.
  </action>
  <verify>
    - Schema deploys with toolCache table
    - First tool call shows "Cache MISS"
    - Same query within TTL shows "Cache HIT"
    - Different query params miss cache
    - search_web cache expires after 15 min
    - search_knowledge cache lasts 24 hours
  </verify>
  <done>Tool result caching reduces redundant API calls and improves response time</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Context management optimizations for long conversations:
    - **Conversation summarization** (12+ messages triggers compression, keeps 6 recent + summary)
    - **Tool result caching** (5m for cases, 15m for web, 24h for knowledge)
    - Token budget optimization reducing context sent to LLM
    - Cache invalidation support for case mutations
  </what-built>
  <how-to-verify>
    1. Start dev server: `cd v2 && pnpm dev`
    2. Log in and navigate to the chat widget

    **Test Conversation Summarization:**
    3. Have 15+ back-and-forth exchanges with the chatbot
       - Ask various questions (case queries, PERM knowledge, app features)
       - Build up a long conversation history

    4. Check console logs for:
       - "Using summarized context: N messages → M recent + summary"
       - Summarization trigger message

    5. Verify chatbot context:
       - Ask about something from the early conversation
       - Chatbot should remember key facts (via summary)
       - Response should be contextually aware

    6. Check Convex dashboard:
       - Navigate to Data > conversations
       - Verify `summary` field is populated with content
       - Check `messageCountAtSummary` and `tokenCount` values

    **Test Tool Caching:**
    7. Ask "How many cases do I have?"
       - Check console for "Cache MISS: query_cases"

    8. Ask the same question again immediately
       - Check console for "Cache HIT: query_cases"

    9. Ask a different case question (e.g., "Which cases are in PWD?")
       - Check console for "Cache MISS: query_cases" (different params)

    10. Wait 5+ minutes and repeat step 7
        - Should show "Cache MISS" (TTL expired for cases)

    11. Ask about PERM knowledge twice
        - First: "Cache MISS: search_knowledge"
        - Second (same query): "Cache HIT: search_knowledge"

    12. Check Convex dashboard:
        - Navigate to Data > toolCache
        - Verify entries exist with correct TTLs
        - Check expiresAt timestamps are correct
  </how-to-verify>
  <resume-signal>Type "approved" if summarization and caching work correctly. Otherwise describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Schema deploys with summary field on conversations
- [ ] Schema deploys with toolCache table
- [ ] Summarization triggers after 12+ messages
- [ ] Summary stored in conversation.summary field
- [ ] Context sent to LLM is summary + recent (not all messages)
- [ ] Console shows "Cache HIT/MISS" messages
- [ ] Different TTLs work (5m/15m/24h)
- [ ] Repeated queries within TTL hit cache
- [ ] Human verification approved
</verification>

<success_criteria>
- Conversation summarization compresses long chats (12+ messages → 6 recent + summary)
- Tool result caching reduces redundant calls (cache HIT on repeated queries)
- Context management follows industry standards (rolling summary, TTL-based caching)
- No regression in chat functionality
- Human verification confirms all features work correctly
- Phase 27 complete
</success_criteria>

<output>
After completion, create `.planning/phases/27-knowledge-layer/27-05-SUMMARY.md`:

# Phase 27 Plan 05: Context Management Summary

**[One-liner describing what shipped]**

## Accomplishments
- Conversation summarization (12+ messages → 6 recent + summary)
- Tool result caching (5m/15m/24h TTLs by tool type)
- Token budget optimization for long conversations
- Human verification passed

## Files Created/Modified
- `v2/convex/schema.ts` - Added summary field to conversations, toolCache table
- `v2/convex/conversationSummary.ts` - Summary queries, mutations, trigger logic
- `v2/convex/toolCache.ts` - TTL-based tool result caching
- `v2/src/lib/ai/summarize.ts` - Async conversation summarization
- `v2/src/app/api/chat/route.ts` - Context management and cache integration

## Decisions Made
- Rolling summary pattern (not hierarchical) - simpler, sufficient for v1
- 12 message threshold for summarization (industry standard: 10-15)
- Keep 6 most recent messages raw (preserves immediate context)
- TTLs: 5m for case data (may change), 15m for web (fresher), 24h for knowledge (static)
- Cache keyed by query hash (not semantic similarity - simpler, more predictable)

## Issues Encountered
[Problems and resolutions, or "None"]

## Next Step
Phase 27 complete. Ready for Phase 28 (Action Layer).
</output>
